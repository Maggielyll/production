{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `01_materials/labs/2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "cannot find .env file\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "%load_ext dotenv\n",
    "%dotenv \n",
    "# pip install python-dotenv\n",
    "#PRICE_DATA='c:\\Users\\yihan\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRICE_DATA is set to: ../05_src/data/prices/\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = r'C:\\Users\\yihan\\production\\05_src\\.env'\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Access the PRICE_DATA variable\n",
    "price_data_path = os.getenv('PRICE_DATA')\n",
    "\n",
    "print(f\"PRICE_DATA is set to: {price_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Load the environment variable PRICE_DATA\n",
    "price_data_path = os.getenv('PRICE_DATA')\n",
    "\n",
    "# Use glob to find all parquet files in the directory\n",
    "parquet_files = glob(os.path.join(price_data_path, '*.parquet'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date    Open   High     Low   Close  Adj Close   Volume  \\\n",
      "ticker                                                                 \n",
      "HUM    2000-01-03  8.3125  8.375  7.3750  7.5625   6.752523  1287300   \n",
      "HUM    2000-01-04  7.3750  7.875  7.3750  7.6250   6.808330  1238300   \n",
      "HUM    2000-01-05  7.5000  7.875  7.5000  7.8125   6.975746  1096300   \n",
      "HUM    2000-01-06  7.7500  8.250  7.5000  8.1250   7.254777  1026700   \n",
      "HUM    2000-01-07  8.1875  9.125  8.0625  8.7500   7.812839  2419300   \n",
      "\n",
      "             sector            subsector  year  Close_lag_1   returns  \\\n",
      "ticker                                                                  \n",
      "HUM     Health Care  Managed Health Care  2000      10.0000       NaN   \n",
      "HUM     Health Care  Managed Health Care  2000       7.5625  0.008265   \n",
      "HUM     Health Care  Managed Health Care  2000       7.6250  0.024590   \n",
      "HUM     Health Care  Managed Health Care  2000       7.8125  0.040000   \n",
      "HUM     Health Care  Managed Health Care  2000       8.1250  0.076923   \n",
      "\n",
      "        positive_return  target  Close_lag  Adj Close_lag  hi_lo_range  \n",
      "ticker                                                                  \n",
      "HUM                   0     1.0        NaN            NaN       1.0000  \n",
      "HUM                   1     1.0     7.5625       6.752523       0.5000  \n",
      "HUM                   1     1.0     7.6250       6.808330       0.3750  \n",
      "HUM                   1     1.0     7.8125       6.975746       0.7500  \n",
      "HUM                   1     0.0     8.1250       7.254777       1.0625  \n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Load your data (adjust the path to where your data is stored)\n",
    "ddf = dd.read_parquet(r'C:\\Users\\yihan\\production\\05_src\\data\\features\\stock_features.parquet')\n",
    "\n",
    "# Repartition the DataFrame to ensure larger partitions\n",
    "ddf = ddf.repartition(partition_size='100MB')\n",
    "\n",
    "# Add lags for Close and Adj Close\n",
    "ddf['Close_lag'] = ddf['Close'].shift(1)\n",
    "ddf['Adj Close_lag'] = ddf['Adj Close'].shift(1)\n",
    "\n",
    "# Add returns based on Adjusted Close\n",
    "ddf['returns'] = (ddf['Adj Close'] / ddf['Adj Close_lag']) - 1\n",
    "\n",
    "# Add the high-low range for the day\n",
    "ddf['hi_lo_range'] = ddf['High'] - ddf['Low']\n",
    "\n",
    "# Assign the result to dd_feat\n",
    "dd_feat = ddf.persist()\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(dd_feat.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date    Open   High     Low   Close  Adj Close   Volume  \\\n",
      "ticker                                                                 \n",
      "HUM    2000-01-03  8.3125  8.375  7.3750  7.5625   6.752523  1287300   \n",
      "HUM    2000-01-04  7.3750  7.875  7.3750  7.6250   6.808330  1238300   \n",
      "HUM    2000-01-05  7.5000  7.875  7.5000  7.8125   6.975746  1096300   \n",
      "HUM    2000-01-06  7.7500  8.250  7.5000  8.1250   7.254777  1026700   \n",
      "HUM    2000-01-07  8.1875  9.125  8.0625  8.7500   7.812839  2419300   \n",
      "\n",
      "             sector            subsector  year  Close_lag_1   returns  \\\n",
      "ticker                                                                  \n",
      "HUM     Health Care  Managed Health Care  2000      10.0000       NaN   \n",
      "HUM     Health Care  Managed Health Care  2000       7.5625  0.008265   \n",
      "HUM     Health Care  Managed Health Care  2000       7.6250  0.024590   \n",
      "HUM     Health Care  Managed Health Care  2000       7.8125  0.040000   \n",
      "HUM     Health Care  Managed Health Care  2000       8.1250  0.076923   \n",
      "\n",
      "        positive_return  target  Close_lag  Adj Close_lag  hi_lo_range  \\\n",
      "ticker                                                                   \n",
      "HUM                   0     1.0        NaN            NaN       1.0000   \n",
      "HUM                   1     1.0     7.5625       6.752523       0.5000   \n",
      "HUM                   1     1.0     7.6250       6.808330       0.3750   \n",
      "HUM                   1     1.0     7.8125       6.975746       0.7500   \n",
      "HUM                   1     0.0     8.1250       7.254777       1.0625   \n",
      "\n",
      "        rolling_avg_return  \n",
      "ticker                      \n",
      "HUM                    NaN  \n",
      "HUM                    NaN  \n",
      "HUM                    NaN  \n",
      "HUM                    NaN  \n",
      "HUM                    NaN  \n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Now convert the Dask DataFrame to a Pandas DataFrame\n",
    "pandas_df = dd_feat.compute()\n",
    "\n",
    "# Add a rolling average return calculation with a window of 10 days\n",
    "pandas_df['rolling_avg_return'] = pandas_df['returns'].rolling(window=10).mean()\n",
    "\n",
    "# Display the first few rows\n",
    "print(pandas_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "\n",
    "No, it is not necessary to convert to Pandas to calculate the moving average return. Dask can perform rolling computations on Dask DataFrames.\n",
    "\n",
    "\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "Yes, it would have been better to perform the rolling average calculation in Dask. Dask is built to efficiently manage datasets that exceed memory limits and can process data concurrently, making it more effective for larger datasets. Converting to Pandas may result in memory-related challenges if the dataset is substantial, and it diminishes the advantages of Daskâ€™s lazy evaluation and parallel processing features.\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
